#### WORD COUNT ####
Essentially this task can be divided into two parts 

MAPPER :  

It is fairly straight forword the function of the mapper is to iterate over each 

word and assign a value "1" to it. This is the generation of a key and a value pair which will 

act as the input to the reducer function. 

So finally for the input text (input.txt) which is :

hello hi marry lamb 
hi hello lamb little

The Output for the mapper function should be :


hello	1
hi	    1
marry	1
lamb	1
hi	    1
hello	1
lamb	1
little	1

<key : hello> <value: 1> will be the input for the reducer, the reducer will read the key value pairs and aggregate 

the number of occurence for each key, and outputs the results to STDOUT.

Output from the REDUCER: 

hello	2
hi	    2
lamb	2
little	1
marry	1


NOTE : Always execute the MapReduce task on your local system before running it on Hadoop platform: 


(1) cat input.txt
(2) cat input.txt |python3 mapper.py
(3) cat input.txt |python3 mapper.py |sort |python3 reducer.py


Now you can implement the WordCount MapReduce task in HDFS. 


Advantages of Hadoop Streaming:

* any language written map, reduce program can run on the hadoop cluster
* any map/reduce program as long as it follows from the standard input stdin read, write out to the standard output stdout.
* it is easy to debug on a single machine as it follows a pipeline framework
* provides a rich parameter control for job submission

REQUIRED:  Binary required for hadoop streaming 

/home/hdoop/hadoop-3.2.4/share/hadoop/tools/lib/hadoop-streaming-3.2.4.jar
C:/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar

#######################################################################################

(1) hdfs namenode -format

(2) ./start-dfs.sh 
    for windows: start-dfs.cmd

(3) ./start-yarn.sh

(4) hdfs dfs -mkdir -p inputword

(5) hdfs dfs -put /home/hdoop/Desktop/WordCountPython/input.txt inputword
/Users/Khaled/Documents/University/Years/Masters/MIE1628/Tutorials/Tutorial_1_MapReduce/Map-Reduce_Python/Tutorial/WordCount/input.txt

(6) hadoop jar C:\hadoop-3.3.0\share\hadoop\tools\lib\hadoop-streaming-3.3.0.jar -file C:\Users\Khaled\Documents\University\Years\Masters\MIE1628\Tutorials\Tutorial_1_MapReduce\Map-Reduce_Python\Tutorial\WordCount\mapper.py -mapper "C:\Users\Khaled\AppData\Local\Microsoft\WindowsApps\python3.exe C:\Users\Khaled\Documents\University\Years\Masters\MIE1628\Tutorials\Tutorial_1_MapReduce\Map-Reduce_Python\Tutorial\WordCount\mapper.py" -file C:\Users\Khaled\Documents\University\Years\Masters\MIE1628\Tutorials\Tutorial_1_MapReduce\Map-Reduce_Python\Tutorial\WordCount\reducer.py -reducer "C:\Users\Khaled\AppData\Local\Microsoft\WindowsApps\python3.exe C:\Users\Khaled\Documents\University\Years\Masters\MIE1628\Tutorials\Tutorial_1_MapReduce\Map-Reduce_Python\Tutorial\WordCount\reducer.py" -input inputword -output outputcount

(7) hdfs dfs -cat outp/part-00000

To quit:
    /stop-yarn.cmd
    /stop-dfs.cmd